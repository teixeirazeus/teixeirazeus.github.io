<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Automated Image Captioning (Bachelor Thesis) | Thiago Teixeira</title><meta name=keywords content="Python,PyTorch,CNN,LSTM,CRNN,DL,AI"><meta name=description content="Can computers summarize the contents of an image?"><meta name=author content><link rel=canonical href=https://teixeirazeus.github.io/projects/automated-image-captioning/><link crossorigin=anonymous href=/assets/css/stylesheet.min.a72801f0f40a8d7f71aa1cafd1c2f2a993a1f26ca1cfd38fdba65d5b9b0f08a0.css integrity="sha256-pygB8PQKjX9xqhyv0cLyqZOh8myhz9OP26ZdW5sPCKA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://teixeirazeus.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://teixeirazeus.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://teixeirazeus.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://teixeirazeus.github.io/apple-touch-icon.png><link rel=mask-icon href=https://teixeirazeus.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Automated Image Captioning (Bachelor Thesis)"><meta property="og:description" content="Can computers summarize the contents of an image?"><meta property="og:type" content="article"><meta property="og:url" content="https://teixeirazeus.github.io/projects/automated-image-captioning/"><meta property="og:image" content="https://teixeirazeus.github.io/projects/automated-image-captioning/cover.jpg"><meta property="article:section" content="projects"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://teixeirazeus.github.io/projects/automated-image-captioning/cover.jpg"><meta name=twitter:title content="Automated Image Captioning (Bachelor Thesis)"><meta name=twitter:description content="Can computers summarize the contents of an image?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://teixeirazeus.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Automated Image Captioning (Bachelor Thesis)","item":"https://teixeirazeus.github.io/projects/automated-image-captioning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Automated Image Captioning (Bachelor Thesis)","name":"Automated Image Captioning (Bachelor Thesis)","description":"Can computers summarize the contents of an image?","keywords":["Python","PyTorch","CNN","LSTM","CRNN","DL","AI"],"articleBody":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","wordCount":"114","inLanguage":"en","image":"https://teixeirazeus.github.io/projects/automated-image-captioning/cover.jpg","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://teixeirazeus.github.io/projects/automated-image-captioning/"},"publisher":{"@type":"Organization","name":"Thiago Teixeira","logo":{"@type":"ImageObject","url":"https://teixeirazeus.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class="header sticky-header"><nav class=nav><div class=logo><a href=https://teixeirazeus.github.io/ accesskey=h title="Thiago Teixeira (Alt + H)">Thiago Teixeira</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://teixeirazeus.github.io/ title=Home><span>Home</span></a></li><li><a href=https://teixeirazeus.github.io/blog title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://teixeirazeus.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://teixeirazeus.github.io/projects/>Projects</a></div><h1 class=post-title>Automated Image Captioning (Bachelor Thesis)</h1><div class=post-description>Can computers summarize the contents of an image?</div><div class=post-meta>Jan 2021 - May 2021</div></header><figure class=entry-cover><img loading=lazy src=https://teixeirazeus.github.io/projects/automated-image-captioning/cover.jpg alt></figure><div class=post-content><h3 id=-colab-notebookhttpscolabresearchgooglecomdrive1q553uslyw3ho6p1g46soedxos_vmhxfj>ðŸ”— <a href=https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ>Colab Notebook</a><a hidden class=anchor aria-hidden=true href=#-colab-notebookhttpscolabresearchgooglecomdrive1q553uslyw3ho6p1g46soedxos_vmhxfj>#</a></h3><h2 id=description>Description<a hidden class=anchor aria-hidden=true href=#description>#</a></h2><p>In this project, I implemented the paper <strong><a href=https://arxiv.org/abs/1502.03044>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></strong>. The neural network, a combination of <strong>CNN</strong> and <strong>LSTM</strong>, was trained on the <strong>MS COCO</strong> dataset and it learns to generate captions from images.</p><p>As the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.
<img loading=lazy src=/projects/automated-image-captioning/img1.jpg alt="Attention Mechanism"></p><p>Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in <strong>PyTorch</strong> on an <strong>Nvidia GTX 1060</strong> graphics card for over 80 epochs.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://teixeirazeus.github.io/tags/python/>Python</a></li><li><a href=https://teixeirazeus.github.io/tags/pytorch/>PyTorch</a></li><li><a href=https://teixeirazeus.github.io/tags/cnn/>CNN</a></li><li><a href=https://teixeirazeus.github.io/tags/lstm/>LSTM</a></li><li><a href=https://teixeirazeus.github.io/tags/crnn/>CRNN</a></li><li><a href=https://teixeirazeus.github.io/tags/dl/>DL</a></li><li><a href=https://teixeirazeus.github.io/tags/ai/>AI</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://teixeirazeus.github.io/>Thiago Teixeira</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>